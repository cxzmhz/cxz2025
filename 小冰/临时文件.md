```js
parserOptions: {
  project: require('path').join(__dirname, './tsconfig.json')
},
```

```js
!(function (window) {
  const host = 'https://aibeings-vip-int.xiaoice.com',
    url =
      host +
      '/CRTCPreview/8b0a70f45d154fc7a4cd90f0bb772ce0?sign=ZXlKaGJHY2lPaUpJVXpJMU5pSjkuZXlKd1lYbHNiMkZrSWpvaWUxd2lZMjl0Y0dGdWVVbGtYQ0k2WENKbVpqbGxaV1EyWkMweU16TXhMVFEwWm1ZdE9XWmpZUzAzWkRkak1EWXpNREJoWlRsY0lpeGNJbWxrWlc1MGFXWnBZMkYwYVc5dVNXUmNJanBjSWpJell6WTRPV1V6TFRRek1tTXRNVEZsWlMwNE5ETXhMVFprTXpCaE1ESTFOVGN4TUMxcGJuUmxjbUZqZEdsMlpTMWlZV05yWlc1a1hDSXNYQ0owYjJ0bGJrbGtYQ0k2WENKa05qUXpZak00WlRsbU0yUTBaRGhsT0RrNU56STFaVEptWVdGaE9XRm1ZbHdpZlNJc0ltbGhkQ0k2TVRjMU1qVXdPRGs1Tnl3aVpYaHdJam95TmpFMk5UQTRPVGszZlEuZnluZU44b1pKVEhNU085RjZNMVVnbXU4RXVMMXBZbmlyTGhQbGNCZVpTVQ==&isIFrame=1';
  const wrapDiv = document.createElement('div');
  wrapDiv.id = 'xiaoice-streaming-embed';
  const container = document.createElement('div');
  container.id = 'xiaoice-streaming-container';
  const stylesheet = document.createElement('style');
  const clientWidth = document.body.clientWidth;
  stylesheet.innerHTML =
    ' #xiaoice-streaming-embed { z-index: 9999; position: fixed; right: 60px; bottom: 60px; width: 200px; height: 200px; border-radius: 50%; border: 2px solid #fff; box-shadow: 0px 8px 24px 0px rgba(0, 0, 0, 0.12); overflow: hidden; } #xiaoice-streaming-embed.horizontal { height: 366px;  width: calc(366px * 16 / 9); border: 0; border-radius: 12px; box-shadow: 0px 20px 20px 0px rgba(0, 0, 0, 0.10); } #xiaoice-streaming-embed.vertical { height: 680px;  width: calc(680px * 9 / 16); border: 0; border-radius: 12px; box-shadow: 0px 20px 20px 0px rgba(0, 0, 0, 0.10); } #xiaoice-streaming-container { width: 100%; height: 100%; } #xiaoice-streaming-container iframe { width: 100%; height: 100%; border: 0; }';
  const iframe = document.createElement('iframe');
  iframe.allowFullscreen = !1;
  iframe.title = 'Streaming Embed';
  iframe.role = 'dialog';
  iframe.allow = 'microphone';
  iframe.src = url;
  iframe.onload = () => {
    const watchDiv = document.getElementById('xiaoice-streaming-embed');
    const resizeObserver = new ResizeObserver(() => {
      iframe.contentWindow.postMessage({ action: 'resize' }, '*');
    });
    resizeObserver.observe(watchDiv);
  };
  window.addEventListener('message', e => {
    if (e.origin !== host) return;
    if (e.data.action === 'start') {
      if (e.data.ratio === 'horizontal') {
        wrapDiv.classList.toggle('horizontal', true);
      } else if (e.data.ratio === 'vertical') {
        wrapDiv.classList.toggle('vertical', true);
      }
    } else if (e.data.action === 'close') {
      wrapDiv.classList.toggle('horizontal', false);
      wrapDiv.classList.toggle('vertical', false);
    }
  });
  container.appendChild(iframe);
  wrapDiv.appendChild(stylesheet);
  wrapDiv.appendChild(container);
  document.body.appendChild(wrapDiv);
})(globalThis);
```

- https://github.com/livekit/client-sdk-js/tree/main
  这个仓库切出去之后视频断流的问题解决

- copilot 的权限申请

- 熟悉项目的视频文件

- 切换到彭舟的分支看下 vite 的改造

- 找限哥了解下 livekit sdk 的相关内容

- 回溯拷贝 sdk 的理由

- 本地改改项目看看开发环境效果

本周工作：

- 搭建前端开发环境，申请相关权限
- 熟悉数字员工项目和代码
- ASR sdk 与 wix 平台兼容性问题定位与反馈
- 数字播放工作台预览问题处理

下周工作：

- 熟悉 sdk 相关代码
- 交互预览升级 RTCSDK2.0 需求评估与开发

---

---

---

文档中的流程简图需要更新https://aibeings-vip.xiaoice.com/developer-doc/show/164

state.globalImage
state.globalVideo
state.caption
TransparentVideo

了解 webgl

private\SDK\aidigSDK\src\core\SDKEffects.tsx 下的 handleDo 挂了 300 秒，可以用 fetch keep-alive 优化

private\SDK\aidigSDK\src\core\types\types.ts 下的 heartText 的 PING 和 PONG 的值一样，是不是写错了

### asr sdk

- asrsdk 加 sentry 上报，识别前和识别后的相关信息都要上报

- 直接把腾讯 asr 的代码拷贝到本地，改下逻辑，支持在一句话结束的时候返回音频，顺便改下支持给 websocket 传参（要支持降噪）

### rtc 和拷贝的那份的区别

> 先和产品确认下预览和分享链接的权益扣减是怎么计算的

1. 建立 websocket 连接以进入房间的时候，ws 的地址是相同的，2 个 signature 传参不同，sdk 里面是拿的连接上的 shortSign，拷贝的是拿的 cookie 里面的登录信息
2. 启动接口， askStream， getStream 等接口都不一样

- 总结：2 者的 websocket 链接的 url 一样，但是用户认证的参数不一样
- 2 者的 http 请求的接口都不一样，sdk 是通过在请求头里配置 signature 来验证用户的，拷贝的是通过 cookie 验证用户的

# 代办事项

1. 互动卡片和交互 sdk 的预览里面的 ASR 也是自己封装的，这个也需要处理，暂时放到后面迭代处理
1. 互动卡片和交互 sdk 的预览替换 rtc2.0，需要整理下 rtc2.0 里面使用到的接口
1. ASR 使用新的包，添加 sentry，获取音频，上报，同时暴露降噪（noise_threshold）的传参(这个参数可以直接传，不过用户还不知道)，ASR 应该是有对腾讯 sdk 的源码进行修改的，应该主要是改了那些连接后台的域名，同时修改打包方法，上报的时候加一个时间戳 timestamp
1. rtc2.0 的 talk 方法添加 extra 参数，透传 ok
1. rtc2.0 开场白场景的画面上的文字不显示问题修复 ok

1. 找下 livekit 配置 页面隐藏不断流的 room 的入口 ok
1. 找下 rtc2.0 使用的接口的配置

```js
// 还要考虑小程序使用的场景，只有互动名片里面才有小程序，是直接将url上的'appletLoginSessionId'写进cookie的appletLoginSessionId里的也是会做权益的扣减
// 小程序的场景，目前主要的区别是，1. 需要将用户的身份认证信息放到cookie里面；2. 在1.0版本中，创建TRTC实例的时候必须先触发视频播放才可以；3. 几个关键步骤后，小程序需要进行单独上报

// 1.0的websocket地址是一样的，预览的时候token直接传的cookie
// 'rtc-livekit-aibeings-int.xiaoice.cn/rtc' 这个websocket的地址是在start接口里面获取的，然后在livekt-room 组件初始化的时候传入，是算法那边提供的
websocket: 'wss://rtc-livekit-aibeings-int.xiaoice.cn/rtc?access_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJpZGVudGl0eV9kMGQ2NzFiZDA5MjU0MDIxYjM5ZjhjMTI1OWJkY2M4ZSIsImlzcyI6ImxpdmVraXQiLCJuYW1lIjoibmFtZV94IiwidmlkZW8iOnsicm9vbUpvaW4iOnRydWUsInJvb20iOiJSb29tTmFtZV83MDJjMGYyNjkxMDc0MDFiOGRlOWEyZjQ3ZWY5MjI4YyJ9LCJzaXAiOnt9LCJleHAiOjE3NTM0MDg2ODAsImp0aSI6ImlkZW50aXR5X2QwZDY3MWJkMDkyNTQwMjFiMzlmOGMxMjU5YmRjYzhlIn0.uJMxS1nK1G-zyQln9AoPsn3WJOAjGI8zETMOX2_Mig0&auto_subscribe=1&sdk=js&version=2.11.4&protocol=15';

// xhr:

// 获取项目的详细配置（在 SDKEffects.tsx 文件中初始化配置的时候调用）：
// 这个接口SDK1.0使用的就是这个，预览那边使用的则是 /talk/project/get
// 引入路径在private\SDK\LiveKitSDK\src\core\SDKEffects.tsx 的 getConfig 方法
getInteractiveDetail: '/openapi/talk/rtc/get?projectId=42f068fc95774826a68cc4fc2ab5b9e9';

// 通过虚拟人id获取项目id，这个1.0的时候也有，预览的时候一定会有项目id，
// 引入路径在private\SDK\LiveKitSDK\src\core\SDKEffects.tsx 的 initConfigPre 方法
getProjectByHumanId: '/openapi/talk/rtc/project/get';

// livekit 启动接口（RTCInteraction._startRTC 方法调用）：
// 引入路径在private\SDK\LiveKitSDK\src\core\SDKEffects.tsx 的 beginStart 方法
// 1.0 的启动接口有2，分别是web端'/talk/task/rtc/sync/start'和小程序端'/applet/rtc/rtc/sync/start'
// 2.0 是启动之后监听room的状态，然后调用源码的api传入参数驱动数字人
// 1.0 则是调用start接口之前先初始化websocket，然后调用socket的send api去发送数据驱动数字人
startLivekit: '/openapi/talk/task/rtc/livekit/start';

// livekit关闭接口：
// 引入路径在private\SDK\LiveKitSDK\src\core\SDKEffects.tsx 的 useEffect 和 页面关闭之前的回调里面使用
stopLivekit: '/openapi/talk/task/rtc/livekit/stop';

// 上报接口（预览是否不影响，不用上报）：新的上报接口
postLivekitReport: '/openapi/cost/livekit_ptest/event';

// 上报，旧的上报接口，目前也有使用，可以确认下是否可以只留一个上报接口
postCostReport: '/openapi/cost/report';
```

- 1.0 下使用的接口

https://aibeings-vip-int.xiaoice.com/api/v1/talk/task/rtc/sync/start 预览走数字员工的域名 interactive-virtualhuman-int.xiaoice.com/ 分享链接

```js
// 根据虚拟人id获取项目id
sdk： `/openapi/talk/rtc/project/get`
交互sdk 无，直接使用项目id
名片 无，直接使用链接上的项目id
名片小程序 无，直接使用链接上的项目id

// 获取项目的详细配置，这个接口现在新旧版本的sdk使用的是一样的接口，预览使用的应该也可以复用
sdk: '/openapi/talk/rtc/get'
交互sdk： '/talk/project/get'
名片： '/talk/project/get'
名片小程序： 'applet/project/info?projectId='

// websocket接口，需要注意不管是sdk还是预览还是小程序，身份认证信息都是放在token里面使用，但是用tokenType区分不同环境的认证信息
一致： `/openapi/interactive/websocket_v2406?taskId=${taskId}&token=${token}&tokenType=${tokenType}`

// 启动接口
sdk： '/openapi/talk/task/rtc/sync/start'
交互sdk: '/talk/task/rtc/sync/start';
名片：'/talk/task/rtc/sync/start'
名片小程序： '/applet/rtc/rtc/sync/start'

// 询问，2.0中无此类接口
sdk： '/openapi/talk/askStream'
交互sdk： '/talk/faq/askStream'
名片 '/talk/faq/askStream',
名片小程序 `/applet/talk/askStream`,

// 获取流式答案，2.0中无此类接口
sdk: '/openapi/talk/getStreamAnswer'
交互sdk： '/talk/faq/getStreamAnswer'
名片  '/talk/faq/getStreamAnswer',
名片小程序 `/applet/talk/getStreamAnswer`,

// 关闭接口
sdk `/openapi/talk/rtc/stop`
交互sdk '/talk/task/rtc/stop'
名片 '/talk/task/rtc/stop'
名片小程序 '/applet/rtc/rtc/stop',

// 上报接口
sdk '/openapi/cost/report'
交互sdk： '/cost/report'
名片 '/cost/report'
名片小程序 `/datareport/appletUpload` // 小程序上报是有单独的上报时期，与正常的上报不冲突


```

7. 了解下 ARS 自己打包后为什么那样子不能直接使用 new
8. ARS sentry 上报上传的时候是否需要拆分？：不用，直接在一句话结束的时候就上报

```ts
import { arrayBufferToBase64, pcmToWav } from './utils';
 noise_threshold?: number;
 // 慎用，噪音参数阈值，取值范围[-1, 1]，取值越大，判定为噪音情况越大。取值越小，判定为人声情况越大，如果要调，建议 0.333/0.5/0.666 这3个数值
    if(this.config.noise_threshold) {
      recognizerParams.noise_threshold = this.config.noise_threshold;
    }
 // 录音结束
    webAudioSpeechRecognizer.OnRecorderStop = (res: Int8Array)=>{
      if(res.length > 0) {
        const arrayBuffer = new Int8Array(res).buffer;
        const wavData = pcmToWav(arrayBuffer, 16000, 16, 1);
        const baseData = arrayBufferToBase64(wavData);
        const blob = new Blob([wavData], { type: 'audio/wav' });
        const url = URL.createObjectURL(blob);
      }
    };

```

```ts
// global.d.ts
declare global {
  interface Window {
    AsrSDK: any;
  }
}

export {};
```

```ts
// index.ts
import * as Sentry from '@sentry/browser';

Sentry.init({
  dsn: 'https://1e77e25824b8bd05a0cd2fdce33068b8@aic-sentry.xiaoice.cn/8',
  // Setting this option to true will send default PII data to Sentry.
  // For example, automatic IP address collection on events
  sendDefaultPii: true,
});

// Sentry.captureException()
```

9. 集成交互的嵌入 HTML 的图片要替换
10. 交互 sdk 支持分辨率切换： ok
11. asr 里面是不是要给腾讯的 sdk 加个单例模式？

定位不显示的开场白文字问题
自己先写下那个 sdk2.0 兼容代码

- ASR 开发完了，更新了源码包，修改了下源码
- 对话文本不显示的问题也找到了，无限那边修改了代码导致的。
- 刚才集成 SDK 的问题用户说没时间，还没处理，不过用户应该不是专职开发
- 交互 sdk 的分享链接和集成的开发完了，现在在做 livekit2.0 的兼容处理，交互 SDK 和互动名片接口和流程的梳理
  然后产品还加了 3 个小需求，一个时 sdk 预览页面加清晰度切换功能，一个是根据现在集成 sdk 线上问题的处理，产品想更新下示例图片，再看下能不能支撑下集成 sdk 灵活使用的场景更新在文档里；talk 方法支持透传参数

```js
// action 标签： name 是动作名列表， loop 是循环播放次数（默认不传是1，传0就是不播放动作），如果小于0（比如：-1），就是在当前标签控制的语音期间无限循环播放
// emotion 标签： name 是表情名，表示在当前标签控制范围内的语音里展示的表情
`<action name="["hello", "no"]" loop=1><emotion name="happy_M">今天天气真好，</emotion><emotion name="sad_M">晴空万里，</emotion></action>我真的好喜欢，<action name="["shitou", "no"]" loop=1><emotion name="happy_M">你们喜欢这个天气和环境吗？</emotion></action>我反正是很喜欢。`;
```

1.0 的 websocket： wss://interactive-virtualhuman.xiaoice.com/openapi/interactive/websocket_v2406

1. 判断下可不可以复用 character 里面的 playMotionStatic 主动播放表情
2. 了解下为什么 character 里面的 resetTalkMorph 这个方法不起作用
3. character 里面的 playTalk 方法如果要实现语音嵌套动作和表情，需要修改下逻辑

- 修改 3dsdk 的本地测试环境

```js
axios/config 改 env
axios/service 改 subkey
test/index.js 改项目 id 和 env
```

模型中表情通常是通过 morphTarget（变形目标）实现的，而动作是通过骨骼动画实现的。当调用 playEmotions 时，实际上需要设置模型的 morphTargetInfluences 属性，而不是执行动作。

Webkit 537.36 Chrome 122.0.6261.119
LiveKit doesn't seem to be supported on this
browser. Try to update your browser and make sure
no browser extensions are disabling webRTC.

```json
{
  "fps": 15,
  "orbitControls": {
    "enablePan": true,
    "enableZoom": true,
    "enableRotate": false,
    "enableRotateX": false,
    "enableRotateY": false
  },
  "loaders": {
    "gltf": {
      "dracoLoaderPath": "/draco/gltf/"
    },
    "hdr": false
  },
  "camera": {
    "type": "perspective",
    "left": -1,
    "right": 1,
    "top": 2.222,
    "bottom": -1.333,
    "fov": 30,
    "near": 0.1,
    "far": 1000,
    "position": [0, 0.2, 10],
    "focus": [0, 0.2, 0],
    "rotation": [10, 2, 0],
    "zoom": 1.9
  },
  "lights": [
    {
      "type": "DirectionalLight",
      "color": "0xFFFFFF",
      "intensity": 0.2,
      "position": [-5, 5, 5],
      "castShadow": false
    },
    {
      "type": "AmbientLight",
      "color": "0xFFFFFF",
      "intensity": 3,
      "position": [-5, 5, 5],
      "castShadow": false
    }
  ],
  "characterOptions": {
    "blinkMorphNames": ["eyeBlinkLeft", "eyeBlinkRight"],
    "position": [0, 0, 0],
    "scale": [1, 1, 1]
  },
  "toneMappingExposure": 1,
  "morphInfluences": {
    "jawForward": 0.6,
    "jawRight": 0.6,
    "jawLeft": 0.6,
    "jawOpen": 1,
    "mouthClose": 0,
    "mouthFunnel": 1,
    "mouthPucker": 1,
    "mouthRight": 0.6,
    "mouthLeft": 0.6,
    "mouthSmileLeft": 0.6,
    "mouthSmileRight": 0.6,
    "mouthFrownLeft": 0.6,
    "mouthFrownRight": 0.6,
    "mouthDimpleLeft": 0.6,
    "mouthDimpleRight": 0.6,
    "mouthStretchLeft": 0.6,
    "mouthStretchRight": 0.6,
    "mouthRollLower": 0,
    "mouthRollUpper": 1,
    "mouthShrugLower": 0,
    "mouthShrugUpper": 1,
    "mouthPressLeft": 0.6,
    "mouthPressRight": 0.6,
    "mouthLowerDownLeft": 0,
    "mouthLowerDownRight": 0,
    "mouthUpperUpLeft": 1,
    "mouthUpperUpRight": 1
  }
}
```

1. 问三庆 /openapi/talk/queryInteractiveVirtualHuman/byBizId/v2 接口获取的数据中的字段 virtualHumanModeInfo 中的 configLink 和 modelLink 获取的 morphInfluences 数据不一致

- 问三庆有没有开场白能显示表情的示例 ok
- 问三庆，长文本加入表情为什么没有效果
- 问三庆，是不是没法打断正在执行的动作
- configLink 中获取到会少很多表情，导致最后在重置的时候没法找到，有些表情重置失败
- 解决办法，自己从 modelLink 里的 gltf 里面取获取 morphInfluences 数据，需要去重

2. 问产品，每个表情需要做多久？比如表情序列里面有 3 个表情，那每个表情需要展示多久？: 每次只能传一个表情，如果传了下一个表情就过渡到下个表情，如果传了空值就变成默认状态

3. 重置方法 resetTalkMorph 里面需要完善表情的重置逻辑，这里因为第一点说的接口返回的表情列表不一致，导致有些表情没法被重置
4. 开场白的动作总是在说完话之后才执行，而且有时候会触发，有时候不会，这个需要处理，原因是 this.queueActions 被清空了，而且有时候是说完话做动作，有时候是说话的时候做动作，需要处理
5. 表情的逻辑可以借鉴 resetTalkMorph 方法里面的做过渡处理
6. 长文本驱动文本+动作不起作用要看看什么原因
7. 备注：表情处理就是改 mesh.morphTargetInfluences 里面的值，动作处理就是改 queueActions 的值

8. 动作没法执行是不是因为被提前清空了？每次执行完一个动作都会从队列中删除，为什么还要做一次性清空的动作: 可能会有需要某个动作在整个播放过程都保持循环的情况存在，如果这样的话就需要在话说完的时候清空
9. ai 搜一下动作为什么会延迟执行，是不是每次都在 finish 时间后执行？有没有其他执行时机？

10. 问下产品，当前项目的表情模板是产品在运营平台配置的，前端在 configLink 中获取的，是否会有不匹配的问题

rtc-livekit-aibeings.xiaoice.cn

本周工作

- 交互 sdk 预览页面加清晰度切换功能，并更新集成连接使用示例图片
- RTC2.0 talk 方法增加透传参数
- RTC2.0 预览和小程序使用场景接口对齐
- 3dsdk 需求开发

下周工作

- 3dsdk 需求开发提测
- 交互 sdk 预览，互动名片接入 RTC2.0
- 易企聊需求熟悉和预研

说完话嘴型没还原的问题: done
动作没完全展示完全就匆匆结束，因为后面估计接了 idle 动作，所以动作展示不完全
动作跟着语音说完就结束了，没法展示后面的动作: done

- 尝试在会话的动作执行的过程中，如果下一个动作是 idle 动作，先不执行

```ts
const prevActions = this.getWeightActions();
// 如果当前有在播放中的动作，同时此次要播放的动作是从idleActions里面选择来播放的，则不执行任何操作
if (prevActions.length > 0 && fromIdleAction) {
  const prevActionNames = prevActions[0]?.name;
  if (!this.isActionCompleted(prevActionNames)) {
    console.log(
      `当前正在播放中的动作是${prevActions[0].name}，此次要播放的动作是来自idle的动作${name}，所以不执行任何操作`
    );
    return;
  }
}
```

- 如果当前语音有动作，就马上执行动作
- 如果当前语音没有动作，就执行配置的默认语音动作
- 如果当前语音没有动作，也没有配置的默认语音动作，就等上一个语音绑定的动作全部结束后(如果是循环动作，就一直播放)，做 idle 动作
- 如果当前会话的所有语音都结束了，就等最后一个语音绑定的动作结束后，执行 idle 动作
  > 这里如果上个语音动作是无限循环的动作，那么只等最后一个语音延续到当前语音的单个动作做完后就开始执行 idle 动作

公司对外的营销渠道都有哪一些
如果,当前语,音没有,动作，也,没有配置的默认语音,动作，

```js
`<action name="['hello', 'no']" loop=-1>
  <emotion name="happy_M">我现在在说第一部分的第一句话，第一部分的动作是hello和no，无限循环，第一句的表情是happy，</emotion>
  <emotion name="sad_M">现在是第一部分的第二句话，第二句的表情是sad。</emotion>
</action>
<emotion name="happy_M">现在是第二部分，没有配置动作，表情是happy。</emotion>
<action name="["shitou", "bu"]" loop=1>
  <emotion name="happy_M">现在是第三部分，只有一句话，动作是石头和布，循环一次，表情是happy。</emotion>
</action>
现在是第四部分，没有动作和表情。
<action name="["beishou", "bixin"]" loop=1>现在是第五部分，也是最后一部分，动作是背手和比心，循环一次，没有表情。</action>`;
```

- 立即播放动作的备份

```js
playActionImmediately: async (actionList: string[], forceImmediate = false) => {
      if (!actionList?.length) return;

      const character = glbRef.current.getCharacter();

      // 如果需要强制立即播放
      if (forceImmediate) {
        // 中断所有当前动作
        const currentActions = character.getWeightActions();
        for (const action of currentActions) {
          action.break();
          // 直接降低权重以立即停止视觉效果
          action.action.setEffectiveWeight(0);
        }
      }

      // 清空队列
      character.queueActions = [];

      // 设置队列中的后续动作
      if (actionList.length > 1) {
        character.queueActions = actionList.slice(1);
      }

      // 立即播放第一个动作
      const curAction = actionList[0];
      await character.playAction(
        curAction,
        forceImmediate ? 0.1 : 0.6 // 如果是强制立即播放，使用更短的过渡时间
      );
    },
```

1. 切换分辨率的需求要看下是否有使用在对的地方：一个是初始化 RTC 接口的时候，一个是在使用 start 接口的时候：ok，start 接口使用的就是初始化的时候传入的数据
2. 看下 2.0 的接口参数的区别
3. 打点信息

- SDKEffects.ts --> getNewAnswer 方法里面 push 了上报信息
  frameTimeLogList.current.push({
  type: 'query',
  content: text,
  sessionId: sessionId,
  current_time: new Date().getTime()
  })
- SDKEffects.tsz sendMessage 方法里面 push 了上报信息
  if (type !== 'heartbeat') {
  frameTimeLogList.current.push({
  type,
  content: text,
  sessionId: socketId,
  fragment_id: socketFragmentId,
  current_time: new Date().getTime(),
  wait: wait ? true : false
  })
  }
- SDKEffects 1118 行的 judgeSEIPayload 方法里面将之前的打点信息统一通过 sentry 上报了
  //记录峰值时间用来打点
  frameTimeLogList.current.push({
  type,
  content,
  sessionId: id,
  fragment_id,
  current_time: new Date().getTime()
  })
  type === FrameEnum.FIRST_FRAME：即首帧加载回来的时候
  sentry.report?.warning + postCostReport 打点

3. postCostReport 里面的 publishType 是不是都设置成 common
4. 接入之后，回答问题的首句从 onTalkStart 回调里面去取，后面的内容从 onStreamData 里面去取

### 互动名片里面的不同点

#### SDKEffects 里面

1. cardInfo ： 业务逻辑，h5 和小程序会有不用的卡片信息接口
2. 整合了 asr 的逻辑，

- 包括 speakMode 这种持续监听语音或微信点击按钮才会说语音的逻辑
- 语音识别的逻辑是放在`if (type === FrameEnum.LAST_FRAME)`这种逻辑里的，后面要处理成放在 livekit 的回调里，可以参考交互 sdk

3. 空闲 20s 后就会播放结束语
4. 小程序场景下的上报
5. sentry 的上报
6. FrameEnum.FIRST_FRAME 里面的代码放到 onTalkStart 里面处理，上报信息已有的就不做处理了
7. FrameEnum.LAST_FRAME 里面的代码放到 onTalkEnd 函数里面处理，上报信息已有的就不做处理了

#### RTCPreview 里面

1. 有个小窗模式
2. 有个红包雨的功能
3. 视频播放做了自定义: 相关的信息在 onStreamData 这个回调里有返回
4. 挂载元素的外面可以再套一层元素，因为挂载元素的样式计算也是从获取配置信息那个接口拿的
5. 上报的先不管，后面统一处理

6. 有一个 state.liveOrMatt 没法设置，不过这个在 2.0 里面也有，看看可不可以忽略

7. 确认结束语能否正常执行
8. 确认每句说完的结束回调是否有
9. 结束用语的逻辑交互 sdk 里面也要补充

<action name="['hello', 'bixin']" loop=-1><emotion name="happy_M">我现在在说第一部分的第一句话，第一部分的动作是 hello 和 比心，无限循环，第一句的表情是 happy，</emotion><emotion name="sad_M">现在是第一部分的第二句话，第二句的表情是 sad。</emotion></action><action name="["beishou", "bixin"]" loop=-1><emotion name="happy_M">现在是第二部分，动作是背手和比心，无限循环，表情是 happy。</emotion></action><emotion name="happy_M">现在是第三部分，没有配置动作，表情是 happy。</emotion><action name="["shitou", "bu"]" loop=1><emotion name="happy_M">现在是第四部分，只有一句话，动作是石头和布，循环一次，表情是 happy。</emotion></action>现在是第五部分，没有动作和表情。<action name="["beishou", "bixin"]" loop=1>现在是第六部分，也是最后一部分，动作是背手和比心，循环一次，没有表情。</action>

<speak>在<say-as interpret-as="date">2022-12-31</say-as>元旦前，<say-as interpret-as="address">北京</say-as>某中学的语文课堂上同学们都按奈不住激动地心情，等待着<sub alias="元旦">01.01</sub>假期的到来。随着<w>下课铃</w>响起学生们一个个<phoneme alphabet="py" ph="ji2 mang2 de5">急忙地</phoneme>收拾东西。此时<break time="3s"/>班主任<say-as interpret-as="name">单老师</say-as>进来了。</speak>

10. 互动名片的小程序的文字聊天窗口还需要适配
    q=https%3A%2F%2Faibeings-vip-int.xiaoice.com%2Fapplet%3FprojectId%3De26f3e64f6d845549ec4acdf9b323df4
11. 点击通话然后马上挂掉还是会进入的问题
12. 确认获取录音权限的弹窗的大小问题

13. 看下产品测试提的问题
14. 交互 sdk 和互动名片的上报逻辑完善
15. 易企聊需求开发

16. 3dsdk 的录屏给产品看 ok
17. 3dsdk 的表情统一使用连续传多个表情的 api: ok
18. rtc1.0 的语言传参问题 ok
19. rtc2.0 的 tts 模型传参问题，看下供应商的内容是怎么传过去的，数字人不展示的，看看是权益用完了还是 int 环境配置有问题 ok
20. 互动名片的字幕的播报逻辑要改
21. 3D 数字人动作变形的原因是因为有 2 个动作叠加在了一起，比如当前播放的明明是 hello，却还有一个 idle 也在播放，而且 idle 动作的 enable 和 isEffective 都为 true，weight 值为 1
22. 火山的 tts 还不支持 rtc2.0，问下思操后面上正式环境了怎么办

23. 看下 3dsdk 流式播放顺序的问题: ok；更新 asr 和 rtc 的文档；ok；看下产品 rtc 拉流为什么失败；上报日志数据完善；互动名片小程序接口联调；字幕问题；3dsdk 的驱动文本这个 api 要隐藏不对外暴露；ok；demo 里的 ssml 这个文案要改下：ok；

24. 字幕跟不上问题；ok；字幕展示不全问题；ok；日志问题；ok；小程序环境下的 title 修改问题；ok；
    小程序环境下 stop 有没有调用问题；
    int 环境下拉流失败问题；添加日志看看
    rtcsdk 在拉流重试的时候直接挂断，然后再次 start，前面的重试流程不会中断，还会继续，后面重试失败了，会直接触发 error；---测试下

25. 拉取 tts 正则信息的接口还需要一个小程序环境的
26. 前端 rtc2.0 加入 sentry 日志上报
27. 加入视频流加载成功的钩子，
28. 添加音频流加载成功的钩子

801ddeed0cbd46dab4c92ac9d2c67617

e59fcc6832224d7da6a130c6cc817999

sessionId： 4951e913-428f-492a-a81e-e24520748361

宁夏公司的信息
账号： 31404183@qq.com
项目 id： ba888416-f030-11ef-9aeb-830d3f2907db

```js
[
  { name: '中文', pattern: '[\\u4e00-\\u9fa5]', value: 211 },
  { name: '数字', pattern: '\\d', value: 251 },
  { name: '英文', pattern: '[a-zA-Z]{3,15}', value: 140 },
  { name: '符号“', pattern: '“+?(?=.)', value: 90 },
  { name: '符号”', pattern: '”+?(?=.)', value: 90 },
  { name: '符号（', pattern: '（+?(?=.)', value: 87 },
  { name: '符号）', pattern: '）+?(?=.)', value: 106 },
  { name: '符号(', pattern: '\\(+?(?=.)', value: 87 },
  { name: '符号)', pattern: '\\)+?(?=.)', value: 106 },
  { name: '符号.', pattern: '\\.+?(?=.)', value: -34 },
  { name: '符号，', pattern: '，+?(?=.)', value: 206 },
  { name: '符号、', pattern: '、+?(?=.)', value: 259 },
  { name: '符号!', pattern: '\\!+?(?=.)', value: 290 },
  { name: '符号?', pattern: '\\?+?(?=.)', value: 334 },
  { name: '符号？', pattern: '？+?(?=.)', value: 647 },
  { name: '符号;', pattern: ';+?(?=.)', value: 306 },
  { name: '符号。', pattern: '。+?(?=.)', value: 647 },
  { name: '符号！', pattern: '！+?(?=.)', value: 634 },
  { name: '符号 ', pattern: '\\s+?(?=.)', value: 206 },
  { name: '符号@', pattern: '@+?(?=.)', value: 368 },
  { name: '符号%', pattern: '%+?(?=.)', value: 709 },
  { name: '符号$', pattern: '\\$+?(?=.)', value: 465 },
  { name: '符号￥', pattern: '￥+?(?=.)', value: 643 },
  { name: '符号[', pattern: '\\[+?(?=.)', value: 215 },
  { name: '符号]', pattern: '\\]+?(?=.)', value: 197 },
  { name: '符号【', pattern: '【+?(?=.)', value: 162 },
  { name: '符号】', pattern: '】+?(?=.)', value: 131 },
  { name: '符号《', pattern: '《+?(?=.)', value: 134 },
  { name: '符号》', pattern: '》+?(?=.)', value: 50 },
  { name: '符号+', pattern: '\\++?(?=.)', value: 225 },
  { name: '符号-', pattern: '-+?(?=.)', value: 96 },
  { name: '符号……', pattern: '……+?(?=.)', value: 412 },
  { name: '符号:', pattern: ':+?(?=.)', value: 322 },
  { name: '符号：', pattern: '：+?(?=.)', value: 322 },
];
```

乐乐智能
1033ab6862a440feb5d3c1bb83db2e83
42427a39eb0641cc9e78ccb8721d1287

ray73pRs77uWwBSVQAh294oNkJhroX3Y

facc9b4e-bd4c-4e95-b707-4e8f8a5324a6

491f98ba-48bb-468e-909d-cf327394254d

线上客户问题处理
3dsdk 的调整
livekit 的联调和跟测

1033ab6862a440feb5d3c1bb83db2e83
a8f966d620f04d0eb34de64b15aa62eb

## 交互 sdk 和互动名片的 ASR 功能替换为统一的 ASRsdk

### 交互 sdk

#### 主要区别

1. 身份认证时，sdk 使用的是用户传入的 subkey；预览使用的是固定的 subkey，需要确认下这个 sdk 使用的具体是什么人的
2. 预览没有 costasr 接口的调用，即不会去计算扣减权益

### 互动名片

zhangqinghua@ruiinsurance.com

ff39e3e4355a433b91b92dab1997d7f1
5756a67dde3f4a188b860b863c8944ef

1033ab6862a440feb5d3c1bb83db2e83
cb1986912bf744769422051d9457babe

给一个单次执行动作和表情的示例 ok
3dsdk 在手机上加载比较慢的问题
叮咚的声音
看下 threejs 官网配置和本地项目配置对比，是不是跟背景渲染有关

const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
// 立即停止流，只是为了获取权限
stream.getTracks().forEach(track => track.stop());

本周工作

- 交互 sdk，互动名片的跟测
- RTC2.0 上报日志完善
- 3dsdk 后台日志管理
- rtc2.0 客户问题的跟踪和解决，瑞众麦克风授权通知音问题的通过业务方式去规避
- 3dsdk 瑞幸客户问题跟踪解决，3d 模型效果调试

下周工作计划

- 瑞幸 3dsdk 接入持续跟进
- 更新 model_verify.html
- 调研 threejs 能不能优化毛发和光追效果
- rtc 的控制台日志优化，
- 交互 sdk 和名片的 ASR 的替换

## Three.js 与 Blender 渲染效果差异分析

1. **格式转换限制和渲染器限制**

工程文件到 threejs 效果这个环节的效果损失的调研结果如下：
效果损失的原因：

1. GLB 格式限制：GLB 格式不支持 Blender 原生渲染器的高级特性，如毛发系统、复杂的材质节点等，Blender 中的这些高级渲染特性在导出为 GLB 格式时会丢失；
2. Threejs 使用的是 WebGL 渲染器，没有光线追踪能力，追求性能而不是完全的物理精确性，与 Blender 的渲染器不同；

效果损失的补偿措施：

1. 可以在 Blender 中，将毛发，光追等特性烘培到法线贴图、环境光遮蔽贴图和光照贴图等，与 GLB 文件一起导出，前端通过应用贴图效果来模拟对应的特性；
2. threejs 官网没有找到与我们需求完全一致的示例，只有一个模拟光追效果的示例，使用的也是模型+贴图的实现方式
3. 加入贴图后，实际所需要下载的素材资源包也会增大，会影响模型的最终渲染出来的时间

- GLB/glTF 格式限制：GLB 格式不支持 Blender 的高级特性，如毛发系统、复杂的材质节点等，Blender 中的这些高级渲染特性在导出为 GLB 格式时会丢失；
- Threejs 使用的是 WebGL 渲染器，没有光线追踪能力，追求性能而不是完全的物理精确性，与 Blender 的渲染器不同

2. **通过烘培贴图进 glb 模型的方式去一定程度上弥补这种效果**

- 法线贴图---毛发效果
- 环境光遮蔽贴图
- 光照贴图

2. 渲染方式差异

- Blender: 使用光线追踪(Ray Tracing)或路径追踪(Path Tracing)渲染器，可产生物理精确的光照效果
- Three.js: 使用实时栅格化渲染，追求性能而非完全物理精确性

3. 着色器系统差异

- Blender 的 Cycles/EEVEE 渲染器支持复杂着色器网络
- Three.js 使用简化的 PBR 材质系统

### PBR（Physically Based Rendering，基于物理的渲染） 与 Blender 高级效果的差异

Three.js 的 PBR 材质与 Blender 中的材质系统存在差距，主要因为：

1. 渲染管线差异：

- Blender：可使用光线追踪或路径追踪，计算全局光照
- Three.js：使用实时光栅化，计算局部光照

2. 材质系统复杂度：

- Blender：支持复杂的材质节点网络和次表面散射
- Three.js：使用简化的 PBR 模型，优化为实时性能

3. 着色器能力：

- Blender：可创建高度自定义的材质行为
- Three.js：标准 PBR 着色器专为网页性能优化

> 这就是为什么 Blender 中的毛发、皮肤和其他复杂效果在导出到 glTF/GLB 格式后会丢失，因为这些高级效果在标准 PBR 材质模型中没有直接对应项。

1. 前端排查的数据给朱虹 ok
2. 3dsdk 加 sentry 上报 ok
3. sentry 上报的显示的时间格式不对 ok
4. RTC 上报加上 init 的 sessionId 或 requestId ok
5. 名片预览的接口报 500 为什么提示项目 id 错误: 项目 id 错误是在调用/api/v1/talk/project/get 接口的时候报的错: ok sdk 这边需要帮忙加个 client-type: PC 的请求头
6. 使用 threejs 渲染 glb，毛发问题可否修复
7. model_vefify.html 里面引用文件更新
8. 使用 fbxLoader 加载 fbx 文件 ok
9. 3dsdk 页面离开的时候调用 stop
10. 问下算法，request_id 和 session_id 传同样的值有没有问题
11. 3d 模型的动作执行时长问题，threejs 解析出的 glb 模型数据里面的确有每个动作的执行时长，不过工作台只有在预览的时候才会使用 threejs，前端可以在工作台的后台通过 threejs 去解析 glb 包，获取对应的动作信息，然后再根据动作名匹配上展示
12. 小程序的名片中明明已经播放音频了，还是展示提示播放媒体的按钮
13. 名片封面的静音按钮点击了没反应

本周工作

- 3dsdk 加载耗时问题排查+上报日志处理，添加生命周期配合客户业务需求，调研渲染损耗问题，fbx 格式模型渲染效果测试，页面离开自动调用 stop 接口，新增离线模式打包，更新建模使用的 verify_modify
- rtc2.0 控制台日志管理
- 交互 sdk 和互动名片跟测上线
- 线上客户问题对齐
  下周工作
- 3dsdk 模型预加载预研
- 3dsdk 瑞幸业务配合
- 交互 sdk 和易企聊线上问题处理
- RTC2.0 客户问题处理

18. 小程序能不能先加载包，然后再把文件传递到 webview 里面
19. 客户的 videodom 的高度不为偶数的问题，看看 get 接口返回的数据，先看 alpha 是不是 true，是的话看 videoSize 的值是多少，不是的话看 attribute 里面的值
